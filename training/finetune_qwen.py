# åœ¨å¯¼å…¥transformersä¹‹å‰ï¼Œç¦ç”¨TensorFlowå¯¼å…¥ä»¥é¿å…DLLåŠ è½½é”™è¯¯
import os
import sys
import types

# å¿…é¡»åœ¨ä»»ä½•å¯¼å…¥ä¹‹å‰è®¾ç½®è¿™äº›ç¯å¢ƒå˜é‡
os.environ["TRANSFORMERS_NO_TF"] = "1"  # ç¦ç”¨TensorFlowåç«¯
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"  # æŠ‘åˆ¶TensorFlowæ—¥å¿—
os.environ["USE_TF"] = "0"  # æ˜ç¡®ç¦ç”¨TensorFlow

# åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„å‡tensorflowæ¨¡å—æ¥é˜»æ­¢å¯¼å…¥
def create_fake_tensorflow():
    """åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„å‡tensorflowæ¨¡å—ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„å±æ€§"""
    try:
        from importlib.util import ModuleSpec
    except ImportError:
        # Python 3.3 æˆ–æ›´æ—©ç‰ˆæœ¬ï¼Œä½¿ç”¨å…¼å®¹æ–¹å¼
        try:
            from importlib.machinery import ModuleSpec
        except ImportError:
            # å¦‚æœéƒ½ä¸è¡Œï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„å¯¹è±¡ä½œä¸ºspec
            class SimpleSpec:
                def __init__(self, name, loader=None, origin=None, is_package=False):
                    self.name = name
                    self.loader = loader
                    self.origin = origin
                    self.submodule_search_locations = [] if is_package else None
            ModuleSpec = SimpleSpec
    
    # åˆ›å»ºä¸»æ¨¡å—
    fake_tf = types.ModuleType('tensorflow')
    
    # åˆ›å»ºæ¨¡å—è§„èŒƒï¼ˆ__spec__ï¼‰- è¿™æ˜¯å…³é”®ï¼
    class FakeLoader:
        pass
    
    try:
        spec = ModuleSpec(
            name='tensorflow',
            loader=FakeLoader(),
            origin='<fake>',
            is_package=True
        )
    except TypeError:
        # å¦‚æœModuleSpecä¸æ”¯æŒis_packageå‚æ•°ï¼ˆæ—§ç‰ˆæœ¬ï¼‰
        spec = ModuleSpec('tensorflow', FakeLoader(), origin='<fake>')
        spec.submodule_search_locations = []
    
    fake_tf.__spec__ = spec
    fake_tf.__version__ = "2.13.0"
    fake_tf.__file__ = "<fake>"
    fake_tf.__package__ = "tensorflow"
    
    # åˆ›å»ºerrorså­æ¨¡å—
    fake_errors = types.ModuleType('tensorflow.errors')
    try:
        errors_spec = ModuleSpec(
            name='tensorflow.errors',
            loader=FakeLoader(),
            origin='<fake>',
            is_package=False
        )
    except TypeError:
        errors_spec = ModuleSpec('tensorflow.errors', FakeLoader(), origin='<fake>')
    fake_errors.__spec__ = errors_spec
    fake_errors.__file__ = "<fake>"
    fake_errors.__package__ = "tensorflow.errors"
    fake_tf.errors = fake_errors
    
    # åˆ›å»ºmodeling_tf_utilså­æ¨¡å—ï¼ˆtransformersä¼šæ£€æŸ¥è¿™ä¸ªï¼‰
    fake_modeling_tf = types.ModuleType('tensorflow.modeling_tf_utils')
    try:
        modeling_tf_spec = ModuleSpec(
            name='tensorflow.modeling_tf_utils',
            loader=FakeLoader(),
            origin='<fake>',
            is_package=False
        )
    except TypeError:
        modeling_tf_spec = ModuleSpec('tensorflow.modeling_tf_utils', FakeLoader(), origin='<fake>')
    fake_modeling_tf.__spec__ = modeling_tf_spec
    fake_modeling_tf.__file__ = "<fake>"
    fake_modeling_tf.__package__ = "tensorflow.modeling_tf_utils"
    
    # åˆ›å»ºkeraså­æ¨¡å—
    fake_keras = types.ModuleType('tensorflow.keras')
    try:
        keras_spec = ModuleSpec(
            name='tensorflow.keras',
            loader=FakeLoader(),
            origin='<fake>',
            is_package=True
        )
    except TypeError:
        keras_spec = ModuleSpec('tensorflow.keras', FakeLoader(), origin='<fake>')
        keras_spec.submodule_search_locations = []
    fake_keras.__spec__ = keras_spec
    fake_keras.__file__ = "<fake>"
    fake_keras.__package__ = "tensorflow.keras"
    fake_tf.keras = fake_keras
    
    # åˆ›å»ºioå­æ¨¡å—ï¼ˆTensorBoardéœ€è¦è¿™ä¸ªï¼‰
    fake_io = types.ModuleType('tensorflow.io')
    try:
        io_spec = ModuleSpec(
            name='tensorflow.io',
            loader=FakeLoader(),
            origin='<fake>',
            is_package=False
        )
    except TypeError:
        io_spec = ModuleSpec('tensorflow.io', FakeLoader(), origin='<fake>')
    fake_io.__spec__ = io_spec
    fake_io.__file__ = "<fake>"
    fake_io.__package__ = "tensorflow.io"
    
    # åˆ›å»ºgfileå­æ¨¡å—ï¼ˆTensorBoardéœ€è¦tf.io.gfileï¼‰
    fake_gfile = types.ModuleType('tensorflow.io.gfile')
    try:
        gfile_spec = ModuleSpec(
            name='tensorflow.io.gfile',
            loader=FakeLoader(),
            origin='<fake>',
            is_package=False
        )
    except TypeError:
        gfile_spec = ModuleSpec('tensorflow.io.gfile', FakeLoader(), origin='<fake>')
    fake_gfile.__spec__ = gfile_spec
    fake_gfile.__file__ = "<fake>"
    fake_gfile.__package__ = "tensorflow.io.gfile"
    
    # æ·»åŠ gfileçš„å¸¸ç”¨æ–¹æ³•ï¼ˆTensorBoardéœ€è¦è¿™äº›ï¼‰
    def fake_join(*paths):
        """å‡çš„joinæ–¹æ³•ï¼Œä½¿ç”¨os.path.join"""
        return os.path.join(*paths)
    
    def fake_makedirs(path, exist_ok=False):
        """å‡çš„makedirsæ–¹æ³•ï¼Œä½¿ç”¨os.makedirs"""
        os.makedirs(path, exist_ok=exist_ok)
    
    def fake_exists(path):
        """å‡çš„existsæ–¹æ³•ï¼Œä½¿ç”¨os.path.exists"""
        return os.path.exists(path)
    
    def fake_isdir(path):
        """å‡çš„isdiræ–¹æ³•ï¼Œä½¿ç”¨os.path.isdir"""
        return os.path.isdir(path)
    
    def fake_isfile(path):
        """å‡çš„isfileæ–¹æ³•ï¼Œä½¿ç”¨os.path.isfile"""
        return os.path.isfile(path)
    
    def fake_listdir(path):
        """å‡çš„listdiræ–¹æ³•ï¼Œä½¿ç”¨os.listdir"""
        return os.listdir(path)
    
    def fake_remove(path):
        """å‡çš„removeæ–¹æ³•ï¼Œä½¿ç”¨os.remove"""
        if os.path.exists(path):
            os.remove(path)
    
    def fake_rmtree(path):
        """å‡çš„rmtreeæ–¹æ³•ï¼Œä½¿ç”¨shutil.rmtree"""
        import shutil
        if os.path.exists(path):
            shutil.rmtree(path)
    
    # å°†æ–¹æ³•æ·»åŠ åˆ°gfileæ¨¡å—
    fake_gfile.join = fake_join
    fake_gfile.makedirs = fake_makedirs
    fake_gfile.exists = fake_exists
    fake_gfile.isdir = fake_isdir
    fake_gfile.isfile = fake_isfile
    fake_gfile.listdir = fake_listdir
    fake_gfile.remove = fake_remove
    fake_gfile.rmtree = fake_rmtree
    
    fake_io.gfile = fake_gfile
    fake_tf.io = fake_io
    
    # æ·»åŠ ä¸€äº›å¸¸ç”¨çš„å±æ€§ï¼Œé¿å…AttributeError
    fake_tf.Tensor = type('Tensor', (), {})
    fake_tf.Variable = type('Variable', (), {})
    fake_tf.Session = type('Session', (), {})
    
    # æ³¨å†Œåˆ°sys.modulesï¼ˆå¿…é¡»åœ¨å¯¼å…¥transformersä¹‹å‰ï¼‰
    sys.modules['tensorflow'] = fake_tf
    sys.modules['tensorflow.errors'] = fake_errors
    sys.modules['tensorflow.modeling_tf_utils'] = fake_modeling_tf
    sys.modules['tensorflow.keras'] = fake_keras
    sys.modules['tensorflow.io'] = fake_io
    sys.modules['tensorflow.io.gfile'] = fake_gfile
    
    return fake_tf

# å°è¯•åˆ›å»ºå‡æ¨¡å—
try:
    create_fake_tensorflow()
except Exception as e:
    # å¦‚æœåˆ›å»ºå¤±è´¥ï¼Œè‡³å°‘è®¾ç½®ç¯å¢ƒå˜é‡
    print(f"[Warning] Failed to create fake tensorflow module: {e}")
    print("[Info] Will rely on environment variables only.")
    pass

def build_data_collator(tokenizer, max_length):
    """æ„å»ºå¯ pickling çš„ collatorï¼Œé¿å…å¤šè¿›ç¨‹ä¸‹çš„æœ¬åœ°å‡½æ•°é—®é¢˜"""
    def collate(features):
        texts = [f["text"] for f in features]
        encodings = tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=max_length,
            return_tensors="pt",
        )
        labels = encodings["input_ids"].clone()
        encodings["labels"] = labels
        return encodings

    return collate

import json
import torch

# ä½¿ç”¨try-exceptåŒ…è£…transformerså¯¼å…¥ï¼Œæä¾›æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯
try:
    from transformers import (
        AutoTokenizer,
        AutoModelForCausalLM,
        TrainingArguments,
        Trainer,
        DataCollatorForLanguageModeling,
        TrainerCallback
    )
except RuntimeError as e:
    if "DLL load failed" in str(e) or "æ‰¾ä¸åˆ°æŒ‡å®šçš„ç¨‹åº" in str(e):
        print("=" * 60)
        print("é”™è¯¯: Transformersåº“å¯¼å…¥å¤±è´¥ï¼ŒåŸå› æ˜¯TensorFlow DLLåŠ è½½é—®é¢˜")
        print("=" * 60)
        print("\nè§£å†³æ–¹æ¡ˆ:")
        print("1. å¸è½½TensorFlowï¼ˆå¦‚æœä¸éœ€è¦ï¼‰:")
        print("   pip uninstall tensorflow tensorflow-cpu")
        print("\n2. æˆ–è€…é‡æ–°å®‰è£…transformers:")
        print("   pip uninstall transformers")
        print("   pip install transformers==4.35.0")
        print("\n3. æˆ–è€…è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆåœ¨è¿è¡Œè„šæœ¬å‰ï¼‰:")
        print("   set TRANSFORMERS_NO_TF=1")
        print("   python training/finetune_qwen.py")
        print("=" * 60)
        sys.exit(1)
    else:
        raise
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType
)
from datasets import Dataset
from tqdm import tqdm
import argparse
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯ï¼Œé€‚åˆæœåŠ¡å™¨ç¯å¢ƒ
import matplotlib.pyplot as plt
class QwenFinetuneConfig:
    """åƒé—®å¾®è°ƒé…ç½®ç±»"""
    def __init__(self):
        # ä¿®æ”¹ä¸ºæœ¬åœ°æ¨¡å‹è·¯å¾„
        self.model_name = "./Qwen-1_8B-Chat"  
        self.output_dir = "./qwen_finetuned_model"
        self.cache_dir = "./model_cache"
        self.use_lora = True
        self.lora_r = 16  # LoRA rank
        self.lora_alpha = 32  # LoRA alpha
        self.lora_dropout = 0.05
        self.target_modules = ["c_attn"]  # Qwen-1.8B ä½¿ç”¨ c_attn æ¥ç»Ÿä¸€å¤„ç†æŸ¥è¯¢ã€é”®ã€å€¼çš„æŠ•å½±
        
        # è®­ç»ƒé…ç½®
        self.num_epochs = 3
        self.batch_size = 4
        self.gradient_accumulation_steps = 4
        self.learning_rate = 2e-4
        self.warmup_steps = 100
        self.max_length = 2048
        self.save_steps = 500
        self.eval_steps = 500
        self.logging_steps = 100
        
        # é‡åŒ–é…ç½®ï¼ˆQLoRAï¼‰
        self.use_4bit = False  # å…³é—­é‡åŒ–ï¼Œé¿å…å…¼å®¹æ€§é—®é¢˜
        self.bnb_4bit_compute_dtype = "float16"
        self.bnb_4bit_quant_type = "nf4"
        self.use_nested_quant = False
        
        # æ•°æ®é…ç½®
        self.train_data_path = "./data/train_data.jsonl"
        self.eval_data_path = "./data/eval_data.jsonl"


# ==================== æ•°æ®åŠ è½½ ====================
def load_jsonl(file_path):
    """åŠ è½½ JSONL æ ¼å¼çš„è®­ç»ƒæ•°æ®"""
    data = []
    if not os.path.exists(file_path):
        print(f"è­¦å‘Š: æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºç¤ºä¾‹æ•°æ®")
        return []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data


def format_instruction_data(data_item, tokenizer):
    """ä½¿ç”¨å®˜æ–¹ chat template ç”Ÿæˆ Qwen å¯¹è¯æ ¼å¼"""
    system_msg = data_item.get(
        "system",
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çœ¼ç§‘åŒ»ç–—åŠ©æ‰‹ï¼Œèƒ½å¤Ÿæ ¹æ®æ‚£è€…çš„ç—‡çŠ¶æä¾›å‡†ç¡®çš„è¯Šæ–­å»ºè®®ã€‚"
    )
    user_msg = data_item.get("instruction", data_item.get("input", ""))
    assistant_msg = data_item.get("output", data_item.get("response", ""))

    messages = [
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_msg},
        {"role": "assistant", "content": assistant_msg},
    ]
    # add_generation_prompt=False -> åŒ…å«å®Œæ•´æ ‡ç­¾ï¼Œé€‚åˆç›‘ç£å¾®è°ƒ
    formatted = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=False,
    )
    return {"text": formatted}


def prepare_dataset(config, tokenizer):
    """å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†"""
    print("æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®...")
    train_data = load_jsonl(config.train_data_path)
    eval_data = load_jsonl(config.eval_data_path) if os.path.exists(config.eval_data_path) else []
    
    if not train_data:
        print("æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®ï¼Œè¯·å…ˆå‡†å¤‡æ•°æ®æ–‡ä»¶")
        return None, None
    
    # æ ¼å¼åŒ–æ•°æ®
    train_formatted = [format_instruction_data(item, tokenizer) for item in train_data]
    eval_formatted = [format_instruction_data(item, tokenizer) for item in eval_data] if eval_data else None
    
    # åˆ›å»º Dataset
    train_dataset = Dataset.from_list(train_formatted)
    eval_dataset = Dataset.from_list(eval_formatted) if eval_formatted else None
    
    return train_dataset, eval_dataset


def load_model_and_tokenizer(config):
    """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {config.model_name}")
    
    # å°è¯•å¤šç§æ–¹å¼åŠ è½½åˆ†è¯å™¨
    tokenizer = None
    
    # æ–¹å¼1: ç›´æ¥ä½¿ç”¨AutoTokenizer
    try:
        print("å°è¯•ä½¿ç”¨AutoTokenizeråŠ è½½åˆ†è¯å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(
            config.model_name,
            trust_remote_code=True,
            cache_dir=config.cache_dir
        )
    except Exception as e:
        print(f"AutoTokenizeråŠ è½½å¤±è´¥: {e}")
    
    # æ–¹å¼2: å°è¯•ä¸“ç”¨çš„QWenTokenizer
    if tokenizer is None:
        try:
            print("å°è¯•ä½¿ç”¨QWenTokenizeråŠ è½½åˆ†è¯å™¨...")
            from transformers import QWenTokenizer
            tokenizer = QWenTokenizer.from_pretrained(
                config.model_name,
                trust_remote_code=True,
                cache_dir=config.cache_dir
            )
        except Exception as e:
            print(f"QWenTokenizeråŠ è½½å¤±è´¥: {e}")
    
    # å¦‚æœä»ç„¶æ²¡æœ‰æˆåŠŸåŠ è½½åˆ†è¯å™¨
    if tokenizer is None:
        raise RuntimeError("æ— æ³•åŠ è½½åˆ†è¯å™¨ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œç¯å¢ƒé…ç½®")
    
    # è®¾ç½® pad_token - å¯¹é½ Qwen Chat æ¨¡æ¿
    if tokenizer.pad_token is None:
        print("æ­£åœ¨ä¸ºåˆ†è¯å™¨è®¾ç½®pad_token...")
        # ä¼˜å…ˆä½¿ç”¨å·²æœ‰ç‰¹æ®Šç¬¦å·ï¼Œé¿å…æ·»åŠ æ–° tokenï¼ˆQwen è€ç‰ˆæœ¬å¯èƒ½ä¸æ”¯æŒï¼‰
        pad_token_id = None
        pad_token = None

        if getattr(tokenizer, "eos_token_id", None) is not None:
            pad_token_id = tokenizer.eos_token_id
            if isinstance(getattr(tokenizer, "eos_token", None), str):
                pad_token = tokenizer.eos_token
            else:
                token_str = tokenizer.convert_ids_to_tokens(pad_token_id)
                if isinstance(token_str, str):
                    pad_token = token_str

        if pad_token is None and isinstance(getattr(tokenizer, "unk_token", None), str):
            pad_token = tokenizer.unk_token
            pad_token_id = tokenizer.unk_token_id

        # å¦‚æœä»ç„¶æ²¡æœ‰å­—ç¬¦ä¸²å½¢å¼ï¼Œå›é€€ä¸ºåŸºäº id çš„å­—ç¬¦ä¸²è¡¨ç¤º
        if pad_token is None and pad_token_id is not None:
            pad_token = str(pad_token_id)

        # æœ€åå…œåº•ï¼šç›´æ¥ç”¨ id 0 çš„ token å­—ç¬¦ä¸²
        if pad_token is None:
            pad_token_id = 0
            token_str = tokenizer.convert_ids_to_tokens(pad_token_id)
            pad_token = token_str if isinstance(token_str, str) else "<|pad|>"

        tokenizer.pad_token = pad_token
        tokenizer.pad_token_id = pad_token_id
    else:
        # å¦‚æœå·²æœ‰ pad_token ä½†ç¼ºå°‘ pad_token_idï¼Œå…œåº•è®¾ä¸º 0
        if tokenizer.pad_token_id is None:
            tokenizer.pad_token_id = 0
    
    # ç»Ÿä¸€ç¡®ä¿ pad_token_id æœ‰æ•ˆä¸”éè´Ÿï¼Œpad_token ä¸ºå­—ç¬¦ä¸²
    pid = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)
    if pid is None or (isinstance(pid, int) and pid < 0):
        # ä¼˜å…ˆç”¨ eos/eodï¼Œå†å…œåº• 0
        pid = getattr(tokenizer, "eod_id", None)
        if pid is None or (isinstance(pid, int) and pid < 0):
            pid = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else 0
    if not isinstance(pid, int):
        pid = 0
    if pid < 0:
        pid = 0
    tokenizer.pad_token_id = pid

    if not isinstance(tokenizer.pad_token, str):
        token_str = tokenizer.convert_ids_to_tokens(pid)
        tokenizer.pad_token = token_str if isinstance(token_str, str) else str(tokenizer.pad_token)

    # ç¡®ä¿å³ä¾§ paddingï¼Œé€‚é… causal LM
    tokenizer.padding_side = "right"

    # è‹¥æ—  chat_templateï¼Œåˆ™è®¾ç½®ä¸€ä¸ªé€‚ç”¨äº Qwen-Chat æ ¼å¼çš„æ¨¡æ¿
    if not getattr(tokenizer, "chat_template", None):
        tokenizer.chat_template = """{% for message in messages -%}
{% if message['role'] == 'system' -%}<|im_start|>system
{{ message['content'] }}<|im_end|>
{% elif message['role'] == 'user' -%}<|im_start|>user
{{ message['content'] }}<|im_end|>
{% elif message['role'] == 'assistant' -%}<|im_start|>assistant
{{ message['content'] }}<|im_end|>
{% endif -%}
{% endfor -%}{% if add_generation_prompt -%}<|im_start|>assistant
{% endif -%}"""

    print(f"æˆåŠŸåŠ è½½åˆ†è¯å™¨: {type(tokenizer).__name__}")
    print(f"åˆ†è¯å™¨ç‰¹æ®Šæ ‡è®°: pad_token={tokenizer.pad_token}, eos_token={tokenizer.eos_token}")
    
    # åŠ è½½æ¨¡å‹
    model_kwargs = {
        "trust_remote_code": True,
        "cache_dir": config.cache_dir,
        "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
        "device_map": "auto" if torch.cuda.is_available() else None,
    }
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡åŒ–é…ç½®
    if config.use_4bit and config.use_lora:
        try:
            from transformers import BitsAndBytesConfig
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=config.use_4bit,
                bnb_4bit_quant_type=config.bnb_4bit_quant_type,
                bnb_4bit_compute_dtype=getattr(torch, config.bnb_4bit_compute_dtype),
                bnb_4bit_use_double_quant=config.use_nested_quant,
            )
            model_kwargs["quantization_config"] = bnb_config
        except Exception as e:
            print(f"è­¦å‘Š: é‡åŒ–é…ç½®åŠ è½½å¤±è´¥: {e}")
            print("å°†ç»§ç»­ä½¿ç”¨éé‡åŒ–é…ç½®...")
    
    model = None
    # å°è¯•å¤šç§æ–¹å¼åŠ è½½æ¨¡å‹
    try:
        print("å°è¯•ä½¿ç”¨AutoModelForCausalLMåŠ è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            config.model_name,
            **model_kwargs
        )
    except Exception as e:
        print(f"AutoModelForCausalLMåŠ è½½å¤±è´¥: {e}")
    
    # å¦‚æœAutoModelåŠ è½½å¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨Qwenæ¨¡å‹ç±»
    if model is None:
        try:
            print("å°è¯•ä½¿ç”¨QWenLMHeadModelåŠ è½½æ¨¡å‹...")
            from transformers import QWenLMHeadModel
            model = QWenLMHeadModel.from_pretrained(
                config.model_name,
                **model_kwargs
            )
        except Exception as e:
            print(f"QWenLMHeadModelåŠ è½½å¤±è´¥: {e}")
    
    if model is None:
        raise RuntimeError("æ— æ³•åŠ è½½æ¨¡å‹ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œç¯å¢ƒé…ç½®")
    
    print(f"æˆåŠŸåŠ è½½æ¨¡å‹: {type(model).__name__}")
    
    # åº”ç”¨ LoRA
    if config.use_lora:
        print("æ­£åœ¨åº”ç”¨ LoRA é…ç½®...")
        try:
            if config.use_4bit:
                model = prepare_model_for_kbit_training(model)
        except Exception as e:
            print(f"è­¦å‘Š: prepare_model_for_kbit_training å¤±è´¥: {e}")
        
        lora_config = LoraConfig(
            r=config.lora_r,
            lora_alpha=config.lora_alpha,
            target_modules=config.target_modules,
            lora_dropout=config.lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )
        
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
    
    return model, tokenizer


class LossPlotCallback(TrainerCallback):
    """è®­ç»ƒlossè®°å½•å’Œå¯è§†åŒ–å›è°ƒ"""
    def __init__(self, output_dir):
        self.output_dir = output_dir
        self.train_losses = []
        self.eval_losses = []
        self.log_steps = []
        self.eval_steps = []
        self.last_plot_step = 0
        self.plot_interval = 50  # æ¯50æ­¥ç»˜åˆ¶ä¸€æ¬¡ï¼ˆå¯é€‰ï¼Œç”¨äºå®æ—¶ç›‘æ§ï¼‰
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        """è®°å½•è®­ç»ƒloss"""
        if logs is not None:
            if 'loss' in logs:
                self.train_losses.append(logs['loss'])
                self.log_steps.append(state.global_step)
                # å¯é€‰ï¼šå®šæœŸä¿å­˜ä¸­é—´å›¾è¡¨ï¼ˆç”¨äºé•¿æ—¶é—´è®­ç»ƒï¼‰
                if state.global_step - self.last_plot_step >= self.plot_interval:
                    self._plot_intermediate()
                    self.last_plot_step = state.global_step
            if 'eval_loss' in logs:
                self.eval_losses.append(logs['eval_loss'])
                self.eval_steps.append(state.global_step)
    
    def _plot_intermediate(self):
        """ç»˜åˆ¶ä¸­é—´å›¾è¡¨ï¼ˆå¯é€‰ï¼Œç”¨äºé•¿æ—¶é—´è®­ç»ƒï¼‰"""
        if len(self.train_losses) > 10:  # è‡³å°‘10ä¸ªç‚¹æ‰ç»˜åˆ¶
            try:
                plot_finetune_loss_curves(
                    self.train_losses, 
                    self.log_steps,
                    self.eval_losses if len(self.eval_losses) > 0 else None,
                    self.eval_steps if len(self.eval_steps) > 0 else None,
                    self.output_dir,
                    suffix="_intermediate"
                )
            except Exception as e:
                print(f"[Warning] ç»˜åˆ¶ä¸­é—´å›¾è¡¨å¤±è´¥: {e}")
    
    def on_train_end(self, args, state, control, **kwargs):
        """è®­ç»ƒç»“æŸæ—¶ç»˜åˆ¶losså›¾"""
        print("\n" + "="*60)
        print("å¼€å§‹ç”Ÿæˆè®­ç»ƒ Loss æ›²çº¿å›¾...")
        print("="*60)
        
        if len(self.train_losses) > 0:
            print(f"è®­ç»ƒ Loss è®°å½•æ•°: {len(self.train_losses)}")
            print(f"éªŒè¯ Loss è®°å½•æ•°: {len(self.eval_losses)}")
            
            plot_finetune_loss_curves(
                self.train_losses, 
                self.log_steps,
                self.eval_losses if len(self.eval_losses) > 0 else None,
                self.eval_steps if len(self.eval_steps) > 0 else None,
                self.output_dir
            )
            print("Loss æ›²çº¿å›¾ç”Ÿæˆå®Œæˆï¼")
        else:
            print("[Warning] æ²¡æœ‰è®°å½•åˆ°è®­ç»ƒ Lossï¼Œæ— æ³•ç”Ÿæˆå›¾è¡¨")
            print("æç¤º: æ£€æŸ¥ logging_steps è®¾ç½®æ˜¯å¦æ­£ç¡®")
        
        print("="*60 + "\n")


def plot_finetune_loss_curves(train_losses, train_steps, eval_losses=None, eval_steps=None, output_dir=None, suffix=""):
    """
    ç»˜åˆ¶å¾®è°ƒè®­ç»ƒlossæ›²çº¿å›¾
    
    Args:
        train_losses: è®­ç»ƒlossåˆ—è¡¨
        train_steps: è®­ç»ƒæ­¥æ•°åˆ—è¡¨
        eval_losses: éªŒè¯lossåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        eval_steps: éªŒè¯æ­¥æ•°åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        output_dir: è¾“å‡ºç›®å½•
        suffix: æ–‡ä»¶ååç¼€ï¼ˆç”¨äºä¸­é—´å›¾è¡¨ï¼‰
    """
    if len(train_losses) == 0:
        print("[Warning] æ²¡æœ‰è®­ç»ƒlossæ•°æ®ï¼Œæ— æ³•ç»˜åˆ¶å›¾è¡¨")
        return
    
    # åˆ›å»ºå›¾è¡¨ï¼Œä½¿ç”¨æ›´å¤§çš„å°ºå¯¸ä»¥ä¾¿æŸ¥çœ‹
    fig, ax = plt.subplots(figsize=(14, 7))
    
    # ç»˜åˆ¶è®­ç»ƒloss
    ax.plot(train_steps, train_losses, 'b-', label='Training Loss', linewidth=2, alpha=0.8, marker='o', markersize=3)
    
    # å¦‚æœæœ‰éªŒè¯lossï¼Œä¹Ÿç»˜åˆ¶
    if eval_losses is not None and len(eval_losses) > 0:
        ax.plot(eval_steps, eval_losses, 'r-', label='Validation Loss', linewidth=2, alpha=0.8, marker='s', markersize=4)
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    min_train_loss = min(train_losses)
    min_train_step = train_steps[train_losses.index(min_train_loss)]
    info_text = f"Min Train Loss: {min_train_loss:.4f} (Step {min_train_step})"
    
    if eval_losses and len(eval_losses) > 0:
        min_eval_loss = min(eval_losses)
        min_eval_step = eval_steps[eval_losses.index(min_eval_loss)]
        info_text += f"\nMin Eval Loss: {min_eval_loss:.4f} (Step {min_eval_step})"
    
    # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
    ax.set_title('Qwen Fine-tuning - Training Loss Curve', fontsize=16, fontweight='bold', pad=20)
    ax.set_xlabel('Training Steps', fontsize=13)
    ax.set_ylabel('Loss', fontsize=13)
    ax.legend(fontsize=12, loc='best')
    ax.grid(True, alpha=0.3, linestyle='--')
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬æ¡†
    ax.text(0.02, 0.98, info_text, transform=ax.transAxes, 
            fontsize=10, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    filename = f"finetune_loss_curve{suffix}.png"
    if output_dir:
        loss_plot_path = os.path.join(output_dir, filename)
        plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')
        print(f"âœ“ å¾®è°ƒLossæ›²çº¿å›¾å·²ä¿å­˜åˆ°: {loss_plot_path}")
    else:
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"âœ“ å¾®è°ƒLossæ›²çº¿å›¾å·²ä¿å­˜åˆ°: {filename}")
    
    plt.close()
    
    # å¦‚æœå¯ç”¨äº† TensorBoardï¼Œæç¤ºç”¨æˆ·
    if output_dir and os.path.exists(os.path.join(output_dir, "runs")):
        print(f"ğŸ’¡ æç¤º: å¯ä»¥ä½¿ç”¨ TensorBoard æŸ¥çœ‹æ›´è¯¦ç»†çš„è®­ç»ƒæ—¥å¿—:")
        print(f"   tensorboard --logdir {os.path.join(output_dir, 'runs')}")


def train_model(config):
    """è®­ç»ƒæ¨¡å‹"""
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    try:
        model, tokenizer = load_model_and_tokenizer(config)
    except Exception as e:
        print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # å‡†å¤‡æ•°æ®é›†ï¼ˆä¾èµ– tokenizer çš„ chat templateï¼‰
    train_dataset, eval_dataset = prepare_dataset(config, tokenizer)
    if train_dataset is None:
        return
    
    # æ•°æ®æ•´ç†å™¨ï¼šæ”¾åœ¨é¡¶å±‚å‡½æ•°å¤–ï¼Œé¿å… Windows å¤šè¿›ç¨‹ pickling é—®é¢˜
    data_collator = build_data_collator(tokenizer, config.max_length)
    
    # è®­ç»ƒå‚æ•°
    # å°è¯•å¯ç”¨ TensorBoardï¼ˆå¦‚æœå¯ç”¨ä¸”TensorFlowæ¨¡å—å®Œæ•´ï¼‰
    report_to_list = ["none"]
    logging_dir = None
    try:
        import tensorboard
        # æ£€æŸ¥TensorBoardæ˜¯å¦èƒ½æ­£å¸¸å·¥ä½œï¼ˆéœ€è¦tensorflow.io.gfileï¼‰
        try:
            # æµ‹è¯•æ˜¯å¦èƒ½è®¿é—®tensorflow.io.gfile
            import tensorflow as tf
            if hasattr(tf, 'io') and hasattr(tf.io, 'gfile') and hasattr(tf.io.gfile, 'join'):
                report_to_list = ["tensorboard"]
                logging_dir = os.path.join(config.output_dir, "runs")
                print(f"[Info] TensorBoard å·²å¯ç”¨ï¼Œæ—¥å¿—ç›®å½•: {logging_dir}")
            else:
                print("[Warning] TensorBoard æ£€æµ‹åˆ°ä½† tensorflow.io.gfile ä¸å¯ç”¨")
                print("[Info] å°†ç¦ç”¨ TensorBoardï¼Œä»…ä½¿ç”¨ matplotlib ç»˜åˆ¶ loss æ›²çº¿")
        except (AttributeError, ImportError) as e:
            print(f"[Warning] TensorBoard æ£€æµ‹åˆ°ä½†æ— æ³•æ­£å¸¸å·¥ä½œ: {e}")
            print("[Info] å°†ç¦ç”¨ TensorBoardï¼Œä»…ä½¿ç”¨ matplotlib ç»˜åˆ¶ loss æ›²çº¿")
    except ImportError:
        print("[Info] TensorBoard æœªå®‰è£…ï¼Œå°†ä»…ä½¿ç”¨ matplotlib ç»˜åˆ¶ loss æ›²çº¿")
    
    training_args = TrainingArguments(
        output_dir=config.output_dir,
        num_train_epochs=config.num_epochs,
        per_device_train_batch_size=config.batch_size,
        per_device_eval_batch_size=config.batch_size,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        learning_rate=config.learning_rate,
        warmup_steps=config.warmup_steps,
        save_steps=config.save_steps,
        eval_steps=config.eval_steps if eval_dataset else None,
        logging_steps=config.logging_steps,
        evaluation_strategy="steps" if eval_dataset else "no",
        save_strategy="steps",
        load_best_model_at_end=True if eval_dataset else False,
        metric_for_best_model="eval_loss" if eval_dataset else None,
        greater_is_better=False if eval_dataset else None,
        fp16=torch.cuda.is_available(),
        dataloader_num_workers=0,  # Windows ä¸Šé¿å…å¤šè¿›ç¨‹ pickling é—®é¢˜
        save_total_limit=3,
        report_to=report_to_list,  # å¦‚æœ TensorBoard å¯ç”¨åˆ™å¯ç”¨
        logging_dir=logging_dir,  # TensorBoard æ—¥å¿—ç›®å½•
        remove_unused_columns=False,  # ä¿ç•™ text åˆ—ï¼Œäº¤ç»™ collator/tokenizer å¤„ç†
    )
    
    # åˆ›å»ºlosså¯è§†åŒ–å›è°ƒ
    loss_callback = LossPlotCallback(config.output_dir)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        callbacks=[loss_callback],  # æ·»åŠ å›è°ƒ
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_model(config.output_dir)
    tokenizer.save_pretrained(config.output_dir)
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {config.output_dir}")
    
    # å¦‚æœå›è°ƒæ²¡æœ‰è‡ªåŠ¨ç»˜åˆ¶ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰ï¼Œä»è®­ç»ƒå†å²ä¸­æå–å¹¶ç»˜åˆ¶
    if hasattr(trainer.state, 'log_history') and len(trainer.state.log_history) > 0:
        try:
            train_losses = []
            train_steps = []
            eval_losses = []
            eval_steps = []
            
            for log in trainer.state.log_history:
                if 'loss' in log and 'step' in log:
                    train_losses.append(log['loss'])
                    train_steps.append(log['step'])
                if 'eval_loss' in log and 'step' in log:
                    eval_losses.append(log['eval_loss'])
                    eval_steps.append(log['step'])
            
            if len(train_losses) > 0:
                plot_finetune_loss_curves(
                    train_losses, 
                    train_steps,
                    eval_losses if len(eval_losses) > 0 else None,
                    eval_steps if len(eval_steps) > 0 else None,
                    config.output_dir
                )
        except Exception as e:
            print(f"ä»è®­ç»ƒå†å²æå–losså¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä½¿ç”¨LoRAå¾®è°ƒåƒé—®æ¨¡å‹")
    parser.add_argument("--model_name", type=str, default="./Qwen-1_8B-Chat", help="åŸºç¡€æ¨¡å‹åç§°æˆ–è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="./qwen_finetuned_model", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--epochs", type=int, default=3, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=4, help="æ‰¹æ¬¡å¤§å°")
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®å¯¹è±¡
    config = QwenFinetuneConfig()
    config.model_name = args.model_name
    config.output_dir = args.output_dir
    config.num_epochs = args.epochs
    config.batch_size = args.batch_size
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config.model_name):
        print(f"é”™è¯¯: æ¨¡å‹è·¯å¾„ {config.model_name} ä¸å­˜åœ¨")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config.output_dir, exist_ok=True)
    
    # å¼€å§‹è®­ç»ƒ
    train_model(config)


if __name__ == "__main__":
    main()
